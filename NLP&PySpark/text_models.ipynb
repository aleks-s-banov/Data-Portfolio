{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualifications analysis for data jobs\n",
    "#### Aleksandar Banov\n",
    "\n",
    "\n",
    "\n",
    "Problem Overview:\n",
    "Can we predict the job title based on the job_descriptions alone for Data Scientist, Data Analyst, Data Engineer jobs.\n",
    "\n",
    "**This notebook** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Spark \n",
    "- **Load Spark with the respective path** \n",
    "Set path where Spark is installed (differences in between paths with windows / linux) \n",
    "\n",
    "- **Create Spark Session** \n",
    "Either take the existing sesseion or create a new one if there is none. Create session based on the parameters.\n",
    "\n",
    "- **Load SparkContext** \n",
    "Context as main entry point for Spark functionality, which we will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "#findspark.init(\"/usr/local/spark/\")\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import the collections module\n",
    "import collections\n",
    "\n",
    "# Gets an existing :class:`SparkSession` or, if there is no existing one, creates a\n",
    "# new one based on the options set in this builder.\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[8]\") \\\n",
    "   .appName(\"RatingsHistogram\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the datasets and do the preprocecssing steps\n",
    "\n",
    "- **Load CSV file.**\n",
    "Read the CSV file with the preprocessed scraped Karriere data\n",
    "\n",
    "- **Select non-duplicates**\n",
    "Using the pyspark distinct() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **karriere_3_classes** dataset that we scraped from Karriere.at with Selenium. After that we preprocessed and cleaned the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|   job_title|     job_description|\n",
      "+------------+--------------------+\n",
      "|Data Analyst|We are Unito - Au...|\n",
      "|Data Analyst|Group leader: in ...|\n",
      "|Data Analyst|(Senior) Data Ana...|\n",
      "|Data Analyst|Secure informatio...|\n",
      "|Data Analyst|Nice code Valley ...|\n",
      "|Data Analyst|Full-time; Advanc...|\n",
      "|Data Analyst|International Loc...|\n",
      "|Data Analyst|\"Data analyst (m/...|\n",
      "|Data Analyst|Moving Healthcare...|\n",
      "|Data Analyst|With 12 Bachelor ...|\n",
      "|Data Analyst|Data Management /...|\n",
      "|Data Analyst|Win2day is the ga...|\n",
      "|Data Analyst|Are you open to n...|\n",
      "|Data Analyst|\"Data analyst (f/...|\n",
      "|Data Analyst|The right way for...|\n",
      "|Data Analyst|Delivery Hero Aus...|\n",
      "|Data Analyst|Internship: Finan...|\n",
      "|Data Analyst|\"Capgemini Invent...|\n",
      "|Data Analyst|Data analyst; for...|\n",
      "|Data Analyst|Moving Healthcare...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading in the data with the proper options\n",
    "df3 = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .csv(\"karriere_3_classes.csv\")\n",
    "\n",
    "df3.show()\n",
    "\n",
    "# assigining it a proper name\n",
    "data = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting only non-duplicates\n",
    "data = data.distinct()\n",
    "\n",
    "# look at how many jobs we got in total\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the intitial 1300 rows after preprocessing steps we only end up with around 300. This is not quite satisfying as our models would benefit quite a bit from more observations but lets try and build them and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|     job_title|count|\n",
      "+--------------+-----+\n",
      "|  Data Analyst|  178|\n",
      "| Data Engineer|   60|\n",
      "|Data Scientist|   54|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# this is similar to the pandas value_counts() function but we have to spell it out quite a bit more for PySpark\n",
    "data.groupBy(\"job_title\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We notice that  our dataset is also not quite the most balanced one but lets continue further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "We have two columns job_titles (Data Scientist, Data Engineer or a Data Analyst) and a column with the respective job_descriptions\n",
    "\n",
    "We are going to utilize the **MLlib** for this task. Machine learning algorithms do not understand texts so we have to convert them into numeric values during this stage. We need to extract the relevant features and characteristics from the raw data that will act as inputs into our model and will be used in making predictions.\n",
    "\n",
    "These features are in form of a tokenizer, vectorizer, and extractor.\n",
    "\n",
    "The steps look like this:\n",
    "\n",
    "- Tokenizer - RegexTokenizer - which allows more advanced tokenization based on regular expression (regex) matching.\n",
    "- StopWordsRemover - drops all the stop words from the input sequences plus any additionally specified ones.\n",
    "- Extractor - CountVectorizer - which will select the top vocabSize words ordered by term frequency across the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"job_description\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\"] # we add these further stopwords because there's sometimes a direct link to apply\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we encode the label column with the StringIndexer instead of doing it manually. Build the pipeline and fit it to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     job_title|     job_description|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| Data Engineer|Data Engineer (40...|[data, engineer, ...|[data, engineer, ...|(2179,[0,1,2,3,4,...|  1.0|\n",
      "|Data Scientist|(Senior) Data Ana...|[senior, data, an...|[senior, data, an...|(2179,[0,1,2,3,4,...|  2.0|\n",
      "|  Data Analyst|\"Do your life's b...|[do, your, life, ...|[do, your, life, ...|(2179,[0,1,2,3,4,...|  0.0|\n",
      "| Data Engineer|Data Engineer (m/...|[data, engineer, ...|[data, engineer, ...|(2179,[0,1,2,3,4,...|  1.0|\n",
      "|  Data Analyst|\"We are Navax, a ...|[we, are, navax, ...|[we, are, navax, ...|(2179,[0,1,2,3,4,...|  0.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# encodes a string column (job_title) of labels to a column of label indices.\n",
    "label_stringIdx = StringIndexer(inputCol = \"job_title\", outputCol = \"label\")\n",
    "\n",
    "#creates a pipeline object with all the previous steps\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to the documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 209\n",
      "Test Dataset Count: 83\n"
     ]
    }
   ],
   "source": [
    "# split the data and set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Next up is building  and fitting the model. We are going to use Logistic Regression which is a generalized linear model that predicts the probability of the categorical outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|               job_description|   job_title|                   probability|label|prediction|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|Financial data analyst (f/m...|Data Analyst|[0.9938094788656089,0.00336...|  0.0|       0.0|\n",
      "|Senior Analyst (F/M/X)*; FO...|Data Analyst|[0.9897014781985672,0.00594...|  0.0|       0.0|\n",
      "|Senior Project Leader for A...|Data Analyst|[0.9881477222898951,0.00585...|  0.0|       0.0|\n",
      "|Financial Analyst - Revenue...|Data Analyst|[0.9875541052733894,0.00834...|  0.0|       0.0|\n",
      "|Moving Healthcare Forward. ...|Data Analyst|[0.9870463255340176,0.00530...|  0.0|       0.0|\n",
      "|Business Analyst - Recyclin...|Data Analyst|[0.9821995138369595,0.00450...|  0.0|       0.0|\n",
      "|Financial Crime Systems Ana...|Data Analyst|[0.9805037326823108,0.01320...|  0.0|       0.0|\n",
      "|Pallas Capital Advisory AG ...|Data Analyst|[0.9780908789281699,0.01033...|  0.0|       0.0|\n",
      "|(Assistant) Manager (all ge...|Data Analyst|[0.976783225478419,0.016832...|  0.0|       0.0|\n",
      "|Job-ID: 146085; Security An...|Data Analyst|[0.9756987928778273,0.02078...|  0.0|       0.0|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "building lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "#fitting the model\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# calculate the predictions\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"job_description\",\"job_title\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5345124339723426"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#evaluating the model's accuracy \n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The we get an accuracy of a bit over 50% which is not good. Let's see if we can undertake some steps to increase the score of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further feature engineering with TF-IDF\n",
    "\n",
    "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.\n",
    "There are several variants on the definition of term frequency and document frequency. In MLlib TF and IDF are separate  so we have to add them as separete steps. Because both HashingTF and CountVectorizer can be used to generate the term frequency vectors we get rid of the CountVectorizer and substitute it for the HashingTF in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|               job_description|   job_title|                   probability|label|prediction|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|Financial data analyst (f/m...|Data Analyst|[0.9946893163710926,0.00238...|  0.0|       0.0|\n",
      "|Financial Analyst - Revenue...|Data Analyst|[0.9933732996713391,0.00484...|  0.0|       0.0|\n",
      "|Design a piece of future wi...|Data Analyst|[0.9919549235358895,0.00401...|  0.0|       0.0|\n",
      "|Moving Healthcare Forward. ...|Data Analyst|[0.9905219683864447,0.00424...|  0.0|       0.0|\n",
      "|Senior Analyst (F/M/X)*; FO...|Data Analyst|[0.9894460095413148,0.00647...|  0.0|       0.0|\n",
      "|Business Analyst - Recyclin...|Data Analyst|[0.9891197033068023,0.00218...|  0.0|       0.0|\n",
      "|Job-ID: 146085; Security An...|Data Analyst|[0.9884380111637738,0.00949...|  0.0|       0.0|\n",
      "|Senior Project Leader for A...|Data Analyst|[0.98490921502961,0.0059372...|  0.0|       0.0|\n",
      "|International Business Requ...|Data Analyst|[0.9847965462757208,0.01206...|  0.0|       0.0|\n",
      "|Pallas Capital Advisory AG ...|Data Analyst|[0.9815830299247622,0.01032...|  0.0|       0.0|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# HashingTF takes sets of terms and converts those sets into fixed-length feature vectors.\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "# IDF takes feature vectors and scales each feature. It down-weights features which appear frequently in a corpus.\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "# adding the new steps to the old pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "# fitting the pipeline to the data\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# again splitting the data into train/test\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "# instantiate the linear model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# calculate the predictions\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"job_description\",\"job_title\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5525550477773162"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model's accuracy again\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just a few percentage points increase from the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV and tunning the model\n",
    "\n",
    "As we know crossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. In our case with k=5 folds, CrossValidator will generate 5 (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing and evaluate the final score.\n",
    "\n",
    "We use paramGrid to help us construct the parameter grid and tune the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8194553564205611"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a pipeline object with all the previous steps\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# fits and transforms the pipeline to the data\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "# split train/test\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "# instantiate the linear model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# calculate the predictions\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wow! So after tuning the model we got a significant boost in our evaluation score. A score of 82% is not that bad considering that this is a 3-class problem and  the amount of observations we have are below 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced models\n",
    "\n",
    "We decided to try out more advanced models in order to explore what Spark NLP has to offer.\n",
    "\n",
    "First we find spark as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Asus\\\\Documents\\\\spark-3.1.3-bin-hadoop3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "#findspark.init(\"/usr/local/spark/\")\n",
    "findspark.init()\n",
    "\n",
    "#prints the home of spark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize an Spark NLP session and import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 4.0.1\n",
      "Apache Spark version: 3.1.3\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "# reassigning spark to sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# for training on GPU\n",
    "sparknlp.start(gpu=True)\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# we use pandas only to visualize the evaluation metrics of the models\n",
    "import pandas as pd\n",
    "\n",
    "# printing the versions for future reference \n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Since we plan to build more advanced models that require deep learning we decided to use a dataset with more observations that our scraped data from Karriere.\n",
    "\n",
    "You can find the dataset that we used [here](https://www.kaggle.com/code/inigoml/data-exploration/data?select=indeed_job_dataset.csv). As the previous one it has 3 labels (Data Scientist, Data Engineer, Data Analyst) and the split bewteeen the jobs is decent enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|     job_title|     job_description|\n",
      "+--------------+--------------------+\n",
      "|data_scientist|\" POSITION SUMMAR...|\n",
      "|data_scientist| What do we need?...|\n",
      "|data_scientist| Validate, analyz...|\n",
      "|data_scientist| Full time, Washi...|\n",
      "|data_scientist| Assist in consul...|\n",
      "|data_scientist| Collecting and c...|\n",
      "|data_scientist| With demand sens...|\n",
      "|data_scientist| Masters degree i...|\n",
      "|data_scientist| Duties   Summary...|\n",
      "|data_scientist| The Department o...|\n",
      "|data_scientist| Salary Commensur...|\n",
      "|data_scientist| Mid Data Scienti...|\n",
      "|data_scientist| WHY CATALINA   ,...|\n",
      "|data_scientist| Located along Fl...|\n",
      "|data_scientist|    Achievement N...|\n",
      "|data_scientist| Implement large-...|\n",
      "|data_scientist| ExxonMobil Resea...|\n",
      "|data_scientist| Working at MIT o...|\n",
      "|data_scientist| We've made a lot...|\n",
      "|data_scientist| Raytheon is an E...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading in the data with the proper options\n",
    "df3 = spark.read \\\n",
    "    .options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .csv(\"DA_DE_DS.csv\")\n",
    "\n",
    "df3.show()\n",
    "\n",
    "# assigining it a proper name\n",
    "data = df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4787"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting only non-duplicates\n",
    "data = data.distinct()\n",
    "\n",
    "# look at how many jobs we got in total\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|     job_title|count|\n",
      "+--------------+-----+\n",
      "|data_scientist| 2155|\n",
      "|  data_analyst| 1486|\n",
      "| data_engineer| 1146|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# this is similar to the pandas value_counts() function but we have to spell it out quite a bit more for PySpark\n",
    "data.groupBy(\"job_title\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split of the data isn't perfect but it will do the trick. Let's now do the test/train split and start building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3338\n",
      "Test Dataset Count: 1449\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set seed for reproducibility\n",
    "(trainDataset, testDataset) = data.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainDataset.count()))\n",
    "print(\"Test Dataset Count: \" + str(testDataset.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the pipeline\n",
    "\n",
    "A standard text classification problem follows these steps:\n",
    "\n",
    "- Text preprocessing and cleaning\n",
    "- Feature engineering\n",
    "- Feature vectorization\n",
    "\n",
    "and finally \n",
    "- Training a model with ML and DL algorithms.\n",
    "\n",
    "In approaching the problem we could have used any text preprocessing and feature vectorization steps that are offered by Spark NLP and then build a model from MLlib. However we wanted to try some of the new pretrained DL models that Spark NLP offers.\n",
    "\n",
    "Below we will utilize the ClassifierDL annotator which uses a deep learning model (DNNs) that has been built inside TensorFlow and supports up to 100 classes. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. \n",
    "\n",
    "The Universal Sentence Encoder part of a group of text embedding methods which are crucial for building any Deep Learning model in Natural Language Processing. The text embedding converts text (words or sentences) into a numerical vector as we always have to boil everything down to a mathematical representation. The Universal Sentence Encoder goes past standard techniques of converting a word to a vector, along with words the context of the whole sentence will be captured in a vector.\n",
    "\n",
    "**Now** before putting the data into a transformer and an annotator we need to get it ready for Spark NLP. **DocumentAssembler()** prepares data into a format that is processable by Spark NLP and is the entry point for every Spark NLP pipeline.\n",
    "\n",
    "This is because the annotator in Spark NLP accepts certain types of columns and outputs new columns in another type. This is why in Spark NLP we constaintly have to **setInputCols** and **setOutputCol**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923,7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside job_description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"job_description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "\n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence\n",
    "\n",
    "# Transformer\n",
    "# downloading pretrained embeddings\n",
    "use = UniversalSentenceEncoder.pretrained()\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# the labels are are in the job_title column\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setLabelColumn(\"job_title\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "\n",
    "# building the pipeline\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        classsifierdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the pipeline to the data\n",
    "use_pipelineModel = use_clf_pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "|     job_title|                                                                 job_description|          result|\n",
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "|  data_analyst| Enterprise Data Governance is a Corporate-wide initiative focused on the est...|  [data_analyst]|\n",
      "| data_engineer|    Design, access and implementation of extensive databases and analytical t...|[data_scientist]|\n",
      "| data_engineer|    Implement their knowledge of cloud, serverless, and hybrid-cloud technolo...|  [data_analyst]|\n",
      "| data_engineer| C.H. Robinson helps drive corporate strategy and provides business value usi...| [data_engineer]|\n",
      "| data_engineer| Castlight is looking for seasoned data engineers who have a penchant for pro...| [data_engineer]|\n",
      "|data_scientist| Onboard with the Grand Rounds team in San Francisco, setup your dev environm...|[data_scientist]|\n",
      "|data_scientist| Our Advanced Analytics Group is a team of experts in data science, engineeri...|[data_scientist]|\n",
      "|  data_analyst| Setup, build and maintain master data records for material, vendors.   Work ...|  [data_analyst]|\n",
      "|  data_analyst| Solomon, the leading performance improvement company for the global energy i...| [data_engineer]|\n",
      "|data_scientist|    Develops long term functional strategy and sets vision concerning short t...|[data_scientist]|\n",
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transforming to the data to make predictions\n",
    "preds = use_pipelineModel.transform(testDataset)\n",
    "\n",
    "preds.select('job_title','job_description',\"class.result\").show(10, truncate = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly use pandas here to create the evaluation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  data_analyst       0.77      0.74      0.75       455\n",
      " data_engineer       0.85      0.76      0.80       347\n",
      "data_scientist       0.80      0.87      0.83       647\n",
      "\n",
      "      accuracy                           0.80      1449\n",
      "     macro avg       0.81      0.79      0.80      1449\n",
      "  weighted avg       0.80      0.80      0.80      1449\n",
      "\n",
      "0.8019323671497585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# get the predictions in a pandas dataframe\n",
    "df= use_pipelineModel.transform(testDataset).select('job_title','job_description',\"class.result\").toPandas()\n",
    "\n",
    "# get the first element of each field of the result column\n",
    "df['result'] = df['result'].apply(lambda x:x[0])\n",
    "\n",
    "# print evaluation matrix and the accuracy score\n",
    "print(classification_report(df.job_title, df.result))\n",
    "print(accuracy_score(df.job_title, df.result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Looks good! And all that took just a few lines of code and we didn't do any text preprocessing or cleaning steps along the way like we had to before with the MLlib model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert pipeline\n",
    "\n",
    "Because the BERT model was the initial plan of the project we decided to implement it here. Lets see if we gain any extra accuracy!\n",
    "\n",
    "We are going to use sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture.\n",
    "\n",
    "We pretty much have exactly the same code as above but here we just use a different transformer - **BertSentenceEmbeddings** instead of the **UniversalSentenceEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L8_512 download started this may take some time.\n",
      "Approximate size to download 149,1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "gain # actual content is inside job_description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"job_description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "\n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence\n",
    "\n",
    "# Transformer\n",
    "# downloading pretrained embeddings\n",
    "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# the labels are are in the job_title column\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setLabelColumn(\"job_title\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "\n",
    "# building the pipeline\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        classsifierdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Here we specified the pretraining that we want the **BertSentenceEmbeddings** to have. We can see that the size of the **sent_small_bert_L8_512** is considerably smaller than what we had above with the UniversalSentenceEncoder which was approx. 1GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the pipeline to the data\n",
    "useBERT_pipelineModel = use_clf_pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "|     job_title|                                                                 job_description|          result|\n",
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "|  data_analyst| Enterprise Data Governance is a Corporate-wide initiative focused on the est...|  [data_analyst]|\n",
      "| data_engineer|    Design, access and implementation of extensive databases and analytical t...|[data_scientist]|\n",
      "| data_engineer|    Implement their knowledge of cloud, serverless, and hybrid-cloud technolo...|  [data_analyst]|\n",
      "| data_engineer| C.H. Robinson helps drive corporate strategy and provides business value usi...| [data_engineer]|\n",
      "| data_engineer| Castlight is looking for seasoned data engineers who have a penchant for pro...| [data_engineer]|\n",
      "|data_scientist| Onboard with the Grand Rounds team in San Francisco, setup your dev environm...| [data_engineer]|\n",
      "|data_scientist| Our Advanced Analytics Group is a team of experts in data science, engineeri...|[data_scientist]|\n",
      "|  data_analyst| Setup, build and maintain master data records for material, vendors.   Work ...|  [data_analyst]|\n",
      "|  data_analyst| Solomon, the leading performance improvement company for the global energy i...| [data_engineer]|\n",
      "|data_scientist|    Develops long term functional strategy and sets vision concerning short t...|[data_scientist]|\n",
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transforming to the data to make predictions\n",
    "preds = useBERT_pipelineModel.transform(testDataset)\n",
    "\n",
    "preds.select('job_title','job_description',\"class.result\").show(10, truncate = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  data_analyst       0.76      0.75      0.76       455\n",
      " data_engineer       0.80      0.82      0.81       347\n",
      "data_scientist       0.84      0.83      0.84       647\n",
      "\n",
      "      accuracy                           0.81      1449\n",
      "     macro avg       0.80      0.80      0.80      1449\n",
      "  weighted avg       0.81      0.81      0.81      1449\n",
      "\n",
      "0.8053830227743272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# get the predictions in a pandas dataframe\n",
    "df= useBERT_pipelineModel.transform(testDataset).select('job_title','job_description',\"class.result\").toPandas()\n",
    "\n",
    "# get the first element of each field of the result column\n",
    "df['result'] = df['result'].apply(lambda x:x[0])\n",
    "\n",
    "# print evaluation matrix and the accuracy score\n",
    "print(classification_report(df.job_title, df.result))\n",
    "print(accuracy_score(df.job_title, df.result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We just got a few extra decimal points to add to the accuracy and neatly round it up to 81%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline that includes text preprocessing with GloVe\n",
    "\n",
    "We saw how easy it was to build, train and predict with Spark NLP pipelines. We didn't need any text preprocessing beforehand. Nevertheless let's try and introduce some into our pipeline and see what are the results. \n",
    "\n",
    "We will at first apply several text preprocessing steps (normalize, remove stopwords and then find lemmas out of words)and then get word embeddings per token (lemma of a token) and then average the word embeddings in each sentence to get a sentence embeddings per row. Then we will utilize Glove word embeddings - GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907,6 KB\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145,3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside job_description column\n",
    "# prepares data into a format that is processable by Spark NLP\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"job_description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizes raw text in document type columns into TokenizedSentence\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Annotator that cleans out tokens. Requires stems - tokens.\n",
    "#Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "# This annotator takes a sequence of strings and drops all the stop words from the input sequences.\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"normalized\")\\\n",
    "    .setOutputCol(\"cleanTokens\")\\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "# Class to find lemmas out of words with the objective of returning a base dictionary word.\n",
    "# Meaning that it retrieves the significant part of a word. \n",
    "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "# Word Embeddings lookup annotator that maps tokens to vectors.\n",
    "word_embeddings = WordEmbeddingsModel().pretrained() \\\n",
    "      .setInputCols([\"document\",'lemma'])\\\n",
    "      .setOutputCol(\"embeddings\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "# Converts the results from WordEmbeddings into sentence or document embeddings\n",
    "# It does that by either summing up or averaging all the word embeddings in a sentence or a document\n",
    "# We have setPoolingStrategy to Average so it does the former\n",
    "embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "# the labels are are in the job_title column\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"job_title\")\\\n",
    "    .setMaxEpochs(5)\\\n",
    "    .setEnableOutputLogs(True)\n",
    "\n",
    "# building the pipeline\n",
    "clf_pipeline_mix = Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    stopwords_cleaner,\n",
    "    lemma,\n",
    "    word_embeddings,\n",
    "    embeddingsSentence,\n",
    "    classsifierdl\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the pipeline to the data\n",
    "clf_pipelineModel_long = clf_pipeline_mix.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "|     job_title|                                                                 job_description|          result|\n",
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "|  data_analyst| Enterprise Data Governance is a Corporate-wide initiative focused on the est...|  [data_analyst]|\n",
      "| data_engineer|    Design, access and implementation of extensive databases and analytical t...| [data_engineer]|\n",
      "| data_engineer|    Implement their knowledge of cloud, serverless, and hybrid-cloud technolo...|  [data_analyst]|\n",
      "| data_engineer| C.H. Robinson helps drive corporate strategy and provides business value usi...| [data_engineer]|\n",
      "| data_engineer| Castlight is looking for seasoned data engineers who have a penchant for pro...| [data_engineer]|\n",
      "|data_scientist| Onboard with the Grand Rounds team in San Francisco, setup your dev environm...|[data_scientist]|\n",
      "|data_scientist| Our Advanced Analytics Group is a team of experts in data science, engineeri...|[data_scientist]|\n",
      "|  data_analyst| Setup, build and maintain master data records for material, vendors.   Work ...|  [data_analyst]|\n",
      "|  data_analyst| Solomon, the leading performance improvement company for the global energy i...|  [data_analyst]|\n",
      "|data_scientist|    Develops long term functional strategy and sets vision concerning short t...|[data_scientist]|\n",
      "+--------------+--------------------------------------------------------------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transforming to the data to make predictions\n",
    "preds = clf_pipelineModel_long.transform(testDataset)\n",
    "\n",
    "preds.select('job_title','job_description',\"class.result\").show(10, truncate = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  data_analyst       0.79      0.66      0.72       455\n",
      " data_engineer       0.79      0.71      0.75       347\n",
      "data_scientist       0.76      0.88      0.82       647\n",
      "\n",
      "      accuracy                           0.77      1449\n",
      "     macro avg       0.78      0.75      0.76      1449\n",
      "  weighted avg       0.77      0.77      0.77      1449\n",
      "\n",
      "0.772256728778468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# get the predictions in a pandas dataframe\n",
    "df= clf_pipelineModel_long.transform(testDataset).select('job_title','job_description',\"class.result\").toPandas()\n",
    "\n",
    "# get the first element of each field of the result column\n",
    "df['result'] = df['result'].apply(lambda x:x[0])\n",
    "\n",
    "# get the first element of each field of the result column\n",
    "print(classification_report(df.job_title, df.result))\n",
    "print(accuracy_score(df.job_title, df.result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished! We got an accuracy of 77% which is a bit lower than the accuracy from the previous pipelines despite the fact that here we did quite a lot of text preprocessing steps. Turns out that while doing text preprocessing we are hurting the model's performance. That could be due to the fact that the Spark NLP models are already trained on unclean text data. And our feature engineering steps may just be including more noise that worsens the model's performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "\n",
    "\n",
    "In building the MLlib models we found usefull:\n",
    "- [Section Article](https://www.section.io/engineering-education/multiclass-text-classification-with-pyspark/)\n",
    "- [MLlib Doc](https://spark.apache.org/docs/1.1.0/mllib-feature-extraction.html)\n",
    "- [Medium Article](https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35)\n",
    "- [Regex](https://www.codepicky.com/regex/#:~:text=Regex%20is%20one%20of%20the,in%20long%20pieces%20of%20text.&text=Regex%20or%20Regexp%2C%20short%20for,typically%20used%20on%20larger%20texts.)\n",
    "\n",
    "\n",
    "In building the Spark NLP models we found usefull:\n",
    "- [Medium Article on Document Assembler](https://medium.com/spark-nlp/spark-nlp-101-document-assembler-500018f5f6b5)\n",
    "- [Text Preprocessing](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb)\n",
    "- [Spark NLP Doc](https://nlp.johnsnowlabs.com/docs/en/quickstart)\n",
    "- [Medium Article](https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32)\n",
    "- [Word Embeddings](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "- [Spark NLP Examples](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.1_Text_classification_examples_in_SparkML_SparkNLP.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the models can predict with a decent certainty what describes a Data Scientist vs Data Analyst or a Data Engineering role. Let us further investigate the 2 datasets that we have - one we scraped, the other we combined and cleaned with PySpark. Please check up next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
